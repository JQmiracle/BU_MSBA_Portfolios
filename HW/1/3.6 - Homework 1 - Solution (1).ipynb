{"cells":[{"cell_type":"markdown","metadata":{"id":"U8YOCpyn-rKa"},"source":["#**HOMEWORK 1: BLUE BIKE TRIP DURATION PREDICTION (Total: / 25 points)**"]},{"cell_type":"markdown","source":["Context: You have received some data from Blue Bikes (the Boston Bikesharing Service). They have asked you to provide a predictive model that can accurately predict how long a particular bike trip will take, from one station to another. The use case is that a customer will open their bike sharing app, and have the option to select a starting and ending station, and receive back a predicted trip duration (in seconds). "],"metadata":{"id":"fxAF2jbRA3WY"}},{"cell_type":"markdown","metadata":{"id":"sMblm7NpWSY9"},"source":["# *Names: (Student One, Student Two)*"]},{"cell_type":"markdown","metadata":{"id":"r0tuEldE-wqV"},"source":["Make sure that you answer the four questions provided at the front of the notebook (edit the XXX response sections to provide your answers). You probably want to answer these last, after you finish the hands-on portion of the assignment. "]},{"cell_type":"markdown","source":["#**Answers to Written Questions (10 points)**"],"metadata":{"id":"PTPJWFj8BT1r"}},{"cell_type":"markdown","source":["**Question 1: What features did you opt to keep, and which did you discard? Why? (2.5 points)**"],"metadata":{"id":"0TFJpxfxBWVt"}},{"cell_type":"markdown","source":["The names of the stations are probably not useful, given you have IDs and also latitudes and longitudes. If you keep these and parsed the text, etc., that was probably overkill and unnecessarily complicates the model. \n","\n","Most imporatantly, however: you should not have used the \"stop time\" in your model! If you did so, that's an information leakage problem. Most trips last less than one hour. So, if you give your model both the start and stop minute, it has a very easy time figuring out that it can take the second value minus the first (or 60+ the second minus the first) to get a reliable predictor of trip duration. Also, just think about it... if you are in the real world generate one of these predictions, you won't know the stop time at the time you are producing a prediction... \n","\n","If you used the lat and longitude of the ending station, that's okay in my book. It really depends on the use case for the predictions. Will the rider be able to tell you / your system where they are trying to go? If so, its fine for that predictor to enter the model. "],"metadata":{"id":"tEFz7uOIBaMd"}},{"cell_type":"markdown","source":["**Question 2: What transformations did you apply to the data, in terms of pre-procesing? Why? What feature engineering can you do here to help the model along? (2.5 points)**"],"metadata":{"id":"fbBY01tlBbio"}},{"cell_type":"markdown","source":["Taking latitude and longitudes of start and stop stations as predictors, these are continuous numeric values. The same goes for the start time. You'd want to 'whiten' the input features before passing them to your model.\n","\n","The most obvious feature engineering option here related to the geolocation data about the docking stations. You can calculate a trip distance variable, rather than have your model try to \"figure it out.\" Haversine distance is a simple option (or even Euclidean distance). Perhaps you went to Google Maps API and pulled in other features of the location? Up to you. "],"metadata":{"id":"OcW93Ho_Bejw"}},{"cell_type":"markdown","source":["**Question 3: What activation functions did you consider for the output layer? Which did you rule out? Why? (2.5 points)**"],"metadata":{"id":"2Uje0SS4Bfbg"}},{"cell_type":"markdown","source":["This is a non-negative value that we are predicting. That means we need an activation function that can return non-negative integers. Linear (no) activation of course will work, but so would a ReLU. You could play with this. You definitely do not want to be using Sigmoid, TanH or Softmax here. "],"metadata":{"id":"5X5JjRzHBjiT"}},{"cell_type":"markdown","source":["**Question 4: What steps did you take to ensure the robustness of your model's performance, e.g., to avoid overfitting, or compatibility with new samples of data? (2.5 points)**"],"metadata":{"id":"sK5i_3CKBkO0"}},{"cell_type":"markdown","source":["Standard idea here: cross-validation, train_test_split, etc. Any approach like this is fine in my view. As far as compatibility with new data, you want to make sure the one-hot encodings don't break the model. If I one-hot encode bike IDs, or station IDs, and use those, it's possible that a new bike or station ID will appear in later observations. Your model won't know how to process them! Think about it, if some bike IDs don't show up in the hold out sample, or if new ones do, when I pre-process the IDs to one-hot encode them, my resulting matrix will have a different number of columns than yours did. That means I have a different number of features. That means your model will throw an error when I try and pass the data for a prediction (or, you'll get lucky, the numbers will line up, but the prediction will be terrible, because the meaning of the features has changed). "],"metadata":{"id":"UC34r4rJBnVm"}},{"cell_type":"markdown","metadata":{"id":"jF365-n0EC-q"},"source":["#*Import and Pre-process Data*"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"dXcPWdFEcpgl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644435789809,"user_tz":300,"elapsed":6776,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"}},"outputId":"5f34a974-cb94-48e3-e312-a9906cdd87d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting haversine\n","  Downloading haversine-2.5.1-py2.py3-none-any.whl (6.1 kB)\n","Installing collected packages: haversine\n","Successfully installed haversine-2.5.1\n"]}],"source":["!pip install haversine"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"a2ab2j0wz3oA","executionInfo":{"status":"ok","timestamp":1644435798800,"user_tz":300,"elapsed":4618,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import utils\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","import requests\n","import json\n","from haversine import haversine\n","\n","bluebikes = pd.read_csv('https://raw.githubusercontent.com/gburtch/BA865-2022/main/Week%203/datasets/bluebikes_sample.csv')\n","\n","# This function MUST return a pair of objects (predictors, labels, in that order) as numpy arrays.\n","def processData(data, makeDist='Haversine'):\n","    \n","    startmin = []\n","    for i in range(len(data)):\n","        startmin.append(int(data.loc[i,'starttime'].split(\":\")[0]))\n","\n","    data['startmin'] = startmin\n","    usertype_bin = np.where(data['usertype']=='Subscriber',1,0)\n","\n","    # This will throw away string variables.\n","    data = data.select_dtypes([np.number])\n","    \n","    # Here is how we could one-hot encode our data. \n","    gender_onehot = utils.to_categorical(data['gender'])\n","\n","    # You will hopefully have realized that you need to be very careful with the features you choose to keep in your model!!\n","    # It's quite possible that you will encounter a new bike ID in the holdout sample that did not appear in the training data.\n","    # The same can happen with station IDs (though it's less likely). \n","    # If this happens, it will produce an error when your code is executed, unless it's handled very carefully. \n","    # These identifiers do not seem to add a lot of value in prediction anyway, so I did not use them.  \n","    end_station_onehot = utils.to_categorical(data['end station id'])\n","    start_station_onehot = utils.to_categorical(data['start station id'])\n","    bike_onehot = utils.to_categorical(data['bikeid'])\n","    \n","    # If we are doing the distance construction, can either use Haversine, or try Open Street Maps.\n","    # I am doing some feature engineering here; use lat and lng to construct travel distances / durations. \n","    if makeDist == 'Haversine':\n","        \n","        # First, let's get the list of unique pairs of start and end station coordinates. \n","        # That way we don't need to duplicate effort.\n","        station_cols = data.loc[:,['start station latitude','start station longitude','end station latitude','end station longitude']]\n","        unique_pairs = station_cols.value_counts(ascending=True).reset_index(name='count').to_numpy()[:,0:4]\n","        \n","        # Put distances for unique pairs into a dictionary for lookups.\n","        # In theory we could do this for pairs in 'either' direction.\n","        dist = {}\n","        for i in range(len(unique_pairs)):\n","            dist[tuple(unique_pairs[i,:])] = haversine(station_cols.loc[i,['start station latitude','start station longitude']],\n","                                                       station_cols.loc[i,['end station latitude','end station longitude']])\n","\n","        hav_dist = []\n","        for i in range(len(data)):\n","            index = data.loc[i,['start station latitude','start station longitude','end station latitude','end station longitude']]\n","            station_dist = dist[tuple(index)]\n","            hav_dist.append(station_dist)\n","\n","        data['hav_dist'] = hav_dist\n","        predictors_cont = data[['start station latitude','start station longitude','end station latitude','end station longitude','birth year','startmin','hav_dist']].to_numpy()\n","        \n","    # Here we are doing trip duration queries using OpenStreetMaps\n","    elif makeDist == 'OSM':\n","        \n","        dist = []\n","        for i in range(len(data)):\n","    \n","            url = f\"http://router.project-osrm.org/route/v1/car/{data.loc[i,['start station longitude']].to_numpy()[0]},{data.loc[i,['start station latitude']].to_numpy()[0]};{data.loc[i,['end station longitude']].to_numpy()[0]},{data.loc[i,['end station latitude']].to_numpy()[0]}?overview=false\"\n","            # call the OSMR API\n","            r = requests.get(url)\n","\n","            # then you load the response using the json libray\n","            # by default you get only one alternative so you access 0-th element of the `routes`\n","            routes = json.loads(r.content)\n","            shortest_duration = routes.get(\"routes\")[0]['duration']\n","            dist.append(shortest_duration)\n","        \n","        data['travel_time'] = dist\n","        predictors_cont = data[['start station latitude','start station longitude','end station latitude','end station longitude','birth year','startmin','travel_time']].to_numpy()\n","    else:\n","        predictors_cont = data[['start station latitude','start station longitude','end station latitude','end station longitude','birth year','startmin']].to_numpy()\n","\n","    # Pulling out continuous predictors, and 'normalizing' them.\n","    # You could also accomplish this with a BatchNormalization() layer in your model.\n","    predictors_cont = np.subtract(predictors_cont,np.mean(predictors_cont,axis=0).reshape(1,predictors_cont.shape[1]))\n","    predictors_cont = np.divide(predictors_cont,np.std(predictors_cont,axis=0).reshape(1,predictors_cont.shape[1]))\n","\n","    # Putting everything back together.\n","    data = np.concatenate((data[['tripduration']].to_numpy(),predictors_cont,usertype_bin.reshape(len(usertype_bin),1),gender_onehot),axis=1) #bike_onehot,start_station_onehot,end_station_onehot\n","    \n","    # Create the labels vector and the matrix of predictors.\n","    labels = data[:,0]\n","    predictors = data[:,1:]\n","    \n","    train_labels = labels\n","    train_predictors = predictors\n","\n","    return train_predictors, train_labels\n"]},{"cell_type":"markdown","metadata":{"id":"vhnwnEtEOeXd"},"source":["#*Specify Your Neural Network Architecture, Process Your Sample*"]},{"cell_type":"markdown","metadata":{"id":"8KZXFE1fWMcj"},"source":["Calling the data pre-processing function on the sample."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"HToKOpiSoZmf","executionInfo":{"status":"ok","timestamp":1644435825894,"user_tz":300,"elapsed":16190,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"}}},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","predictors, labels = processData(bluebikes)"]},{"cell_type":"markdown","metadata":{"id":"RGYaiNdpoako"},"source":["Specifying my Neural Network's structure. Note that the important thing for performance with this model actually comes down to its depth! It turns out that width isn't that important here. \n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"8JBFs2OXdZzU","executionInfo":{"status":"ok","timestamp":1644435971785,"user_tz":300,"elapsed":114,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"}}},"outputs":[],"source":["def build_model():\n","    model = keras.Sequential([\n","        layers.BatchNormalization(),\n","        layers.Dense(10, activation=\"relu\"),\n","        layers.BatchNormalization(),\n","        layers.Dense(10, activation=\"relu\"),\n","        layers.BatchNormalization(),\n","        layers.Dense(10, activation=\"relu\"),\n","        layers.BatchNormalization(),\n","        layers.Dense(10, activation=\"relu\"),\n","        layers.BatchNormalization(),\n","        layers.Dense(1)\n","    ])\n","    model.compile(optimizer=\"rmsprop\", loss=\"mae\", metrics=[\"mae\"])\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"5etiTCpuMQH5"},"source":["#*Train Your Neural Network Here*"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"imfRiESllpKm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644436127858,"user_tz":300,"elapsed":153062,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"}},"outputId":"316961c8-0e05-466c-aaa4-ca01e14ffbe7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing fold #: 0\n","Epoch 1/150\n","60/60 [==============================] - 3s 11ms/step - loss: 850.1215 - mae: 850.1215 - val_loss: 849.3384 - val_mae: 849.3384\n","Epoch 2/150\n","60/60 [==============================] - 0s 8ms/step - loss: 849.7796 - mae: 849.7796 - val_loss: 849.0845 - val_mae: 849.0845\n","Epoch 3/150\n","60/60 [==============================] - 0s 7ms/step - loss: 849.3654 - mae: 849.3654 - val_loss: 848.7019 - val_mae: 848.7019\n","Epoch 4/150\n","60/60 [==============================] - 0s 7ms/step - loss: 848.8765 - mae: 848.8765 - val_loss: 848.2066 - val_mae: 848.2066\n","Epoch 5/150\n","60/60 [==============================] - 0s 7ms/step - loss: 848.3134 - mae: 848.3134 - val_loss: 847.6241 - val_mae: 847.6241\n","Epoch 6/150\n","60/60 [==============================] - 0s 7ms/step - loss: 847.6766 - mae: 847.6766 - val_loss: 846.9727 - val_mae: 846.9727\n","Epoch 7/150\n","60/60 [==============================] - 0s 8ms/step - loss: 846.9662 - mae: 846.9662 - val_loss: 846.3223 - val_mae: 846.3223\n","Epoch 8/150\n","60/60 [==============================] - 0s 6ms/step - loss: 846.1826 - mae: 846.1826 - val_loss: 845.4525 - val_mae: 845.4525\n","Epoch 9/150\n","60/60 [==============================] - 0s 7ms/step - loss: 845.3260 - mae: 845.3260 - val_loss: 844.6389 - val_mae: 844.6389\n","Epoch 10/150\n","60/60 [==============================] - 0s 7ms/step - loss: 844.3964 - mae: 844.3964 - val_loss: 843.6997 - val_mae: 843.6997\n","Epoch 11/150\n","60/60 [==============================] - 0s 7ms/step - loss: 843.3937 - mae: 843.3937 - val_loss: 842.6660 - val_mae: 842.6660\n","Epoch 12/150\n","60/60 [==============================] - 0s 8ms/step - loss: 842.3181 - mae: 842.3181 - val_loss: 841.6886 - val_mae: 841.6886\n","Epoch 13/150\n","60/60 [==============================] - 0s 7ms/step - loss: 841.1701 - mae: 841.1701 - val_loss: 840.4769 - val_mae: 840.4769\n","Epoch 14/150\n","60/60 [==============================] - 0s 7ms/step - loss: 839.9490 - mae: 839.9490 - val_loss: 839.3272 - val_mae: 839.3272\n","Epoch 15/150\n","60/60 [==============================] - 0s 6ms/step - loss: 838.6552 - mae: 838.6552 - val_loss: 838.0159 - val_mae: 838.0159\n","Epoch 16/150\n","60/60 [==============================] - 0s 6ms/step - loss: 837.2891 - mae: 837.2891 - val_loss: 836.5649 - val_mae: 836.5649\n","Epoch 17/150\n","60/60 [==============================] - 0s 6ms/step - loss: 835.8502 - mae: 835.8502 - val_loss: 835.3314 - val_mae: 835.3314\n","Epoch 18/150\n","60/60 [==============================] - 0s 7ms/step - loss: 834.3387 - mae: 834.3387 - val_loss: 833.9500 - val_mae: 833.9500\n","Epoch 19/150\n","60/60 [==============================] - 0s 6ms/step - loss: 832.7545 - mae: 832.7545 - val_loss: 832.3788 - val_mae: 832.3788\n","Epoch 20/150\n","60/60 [==============================] - 0s 6ms/step - loss: 831.0980 - mae: 831.0980 - val_loss: 830.7488 - val_mae: 830.7488\n","Epoch 21/150\n","60/60 [==============================] - 0s 7ms/step - loss: 829.3691 - mae: 829.3691 - val_loss: 829.0983 - val_mae: 829.0983\n","Epoch 22/150\n","60/60 [==============================] - 0s 6ms/step - loss: 827.5674 - mae: 827.5674 - val_loss: 827.5637 - val_mae: 827.5637\n","Epoch 23/150\n","60/60 [==============================] - 0s 7ms/step - loss: 825.6934 - mae: 825.6934 - val_loss: 825.7673 - val_mae: 825.7673\n","Epoch 24/150\n","60/60 [==============================] - 0s 7ms/step - loss: 823.7474 - mae: 823.7474 - val_loss: 823.6898 - val_mae: 823.6898\n","Epoch 25/150\n","60/60 [==============================] - 0s 7ms/step - loss: 821.7284 - mae: 821.7284 - val_loss: 821.4735 - val_mae: 821.4734\n","Epoch 26/150\n","60/60 [==============================] - 0s 7ms/step - loss: 819.6372 - mae: 819.6372 - val_loss: 819.3366 - val_mae: 819.3366\n","Epoch 27/150\n","60/60 [==============================] - 0s 7ms/step - loss: 817.4738 - mae: 817.4738 - val_loss: 817.0837 - val_mae: 817.0837\n","Epoch 28/150\n","60/60 [==============================] - 0s 7ms/step - loss: 815.2377 - mae: 815.2377 - val_loss: 814.7105 - val_mae: 814.7105\n","Epoch 29/150\n","60/60 [==============================] - 0s 7ms/step - loss: 812.9294 - mae: 812.9294 - val_loss: 812.1442 - val_mae: 812.1442\n","Epoch 30/150\n","60/60 [==============================] - 0s 8ms/step - loss: 810.5488 - mae: 810.5488 - val_loss: 809.4768 - val_mae: 809.4768\n","Epoch 31/150\n","60/60 [==============================] - 0s 7ms/step - loss: 808.0958 - mae: 808.0958 - val_loss: 806.8224 - val_mae: 806.8224\n","Epoch 32/150\n","60/60 [==============================] - 0s 7ms/step - loss: 805.5707 - mae: 805.5707 - val_loss: 804.2919 - val_mae: 804.2919\n","Epoch 33/150\n","60/60 [==============================] - 0s 7ms/step - loss: 802.9730 - mae: 802.9730 - val_loss: 801.7244 - val_mae: 801.7244\n","Epoch 34/150\n","60/60 [==============================] - 0s 8ms/step - loss: 800.3031 - mae: 800.3031 - val_loss: 798.9975 - val_mae: 798.9975\n","Epoch 35/150\n","60/60 [==============================] - 0s 7ms/step - loss: 797.5605 - mae: 797.5605 - val_loss: 796.4321 - val_mae: 796.4321\n","Epoch 36/150\n","60/60 [==============================] - 0s 6ms/step - loss: 794.7463 - mae: 794.7463 - val_loss: 793.5240 - val_mae: 793.5240\n","Epoch 37/150\n","60/60 [==============================] - 0s 7ms/step - loss: 791.8596 - mae: 791.8596 - val_loss: 790.6375 - val_mae: 790.6375\n","Epoch 38/150\n","60/60 [==============================] - 0s 7ms/step - loss: 788.9007 - mae: 788.9007 - val_loss: 787.7631 - val_mae: 787.7631\n","Epoch 39/150\n","60/60 [==============================] - 0s 6ms/step - loss: 785.8698 - mae: 785.8698 - val_loss: 784.5648 - val_mae: 784.5648\n","Epoch 40/150\n","60/60 [==============================] - 0s 6ms/step - loss: 782.7675 - mae: 782.7675 - val_loss: 780.8060 - val_mae: 780.8060\n","Epoch 41/150\n","60/60 [==============================] - 0s 6ms/step - loss: 779.5931 - mae: 779.5931 - val_loss: 776.6854 - val_mae: 776.6854\n","Epoch 42/150\n","60/60 [==============================] - 0s 6ms/step - loss: 776.3487 - mae: 776.3487 - val_loss: 772.6332 - val_mae: 772.6332\n","Epoch 43/150\n","60/60 [==============================] - 0s 7ms/step - loss: 773.0323 - mae: 773.0323 - val_loss: 768.5629 - val_mae: 768.5629\n","Epoch 44/150\n","60/60 [==============================] - 0s 7ms/step - loss: 769.6447 - mae: 769.6447 - val_loss: 764.5042 - val_mae: 764.5042\n","Epoch 45/150\n","60/60 [==============================] - 0s 6ms/step - loss: 766.1856 - mae: 766.1856 - val_loss: 760.7540 - val_mae: 760.7540\n","Epoch 46/150\n","60/60 [==============================] - 0s 7ms/step - loss: 762.6818 - mae: 762.6818 - val_loss: 757.9713 - val_mae: 757.9713\n","Epoch 47/150\n","60/60 [==============================] - 0s 7ms/step - loss: 759.0656 - mae: 759.0656 - val_loss: 753.6251 - val_mae: 753.6251\n","Epoch 48/150\n","60/60 [==============================] - 0s 6ms/step - loss: 755.3896 - mae: 755.3896 - val_loss: 749.8279 - val_mae: 749.8279\n","Epoch 49/150\n","60/60 [==============================] - 0s 7ms/step - loss: 751.6677 - mae: 751.6677 - val_loss: 744.2115 - val_mae: 744.2115\n","Epoch 50/150\n","60/60 [==============================] - 0s 7ms/step - loss: 747.8672 - mae: 747.8672 - val_loss: 740.9971 - val_mae: 740.9971\n","Epoch 51/150\n","60/60 [==============================] - 0s 8ms/step - loss: 743.9703 - mae: 743.9703 - val_loss: 738.1934 - val_mae: 738.1934\n","Epoch 52/150\n","60/60 [==============================] - 0s 7ms/step - loss: 740.0634 - mae: 740.0634 - val_loss: 734.9703 - val_mae: 734.9703\n","Epoch 53/150\n","60/60 [==============================] - 0s 7ms/step - loss: 736.0461 - mae: 736.0461 - val_loss: 731.4185 - val_mae: 731.4185\n","Epoch 54/150\n","60/60 [==============================] - 0s 8ms/step - loss: 731.9943 - mae: 731.9943 - val_loss: 728.5447 - val_mae: 728.5447\n","Epoch 55/150\n","60/60 [==============================] - 0s 6ms/step - loss: 727.8425 - mae: 727.8425 - val_loss: 722.5858 - val_mae: 722.5858\n","Epoch 56/150\n","60/60 [==============================] - 0s 7ms/step - loss: 723.6562 - mae: 723.6562 - val_loss: 718.3708 - val_mae: 718.3708\n","Epoch 57/150\n","60/60 [==============================] - 0s 6ms/step - loss: 719.3979 - mae: 719.3979 - val_loss: 714.4689 - val_mae: 714.4689\n","Epoch 58/150\n","60/60 [==============================] - 0s 8ms/step - loss: 715.0677 - mae: 715.0677 - val_loss: 711.1384 - val_mae: 711.1384\n","Epoch 59/150\n","60/60 [==============================] - 0s 8ms/step - loss: 710.6544 - mae: 710.6544 - val_loss: 707.1633 - val_mae: 707.1633\n","Epoch 60/150\n","60/60 [==============================] - 0s 8ms/step - loss: 706.1911 - mae: 706.1911 - val_loss: 702.9922 - val_mae: 702.9922\n","Epoch 61/150\n","60/60 [==============================] - 0s 8ms/step - loss: 701.7070 - mae: 701.7070 - val_loss: 697.5628 - val_mae: 697.5629\n","Epoch 62/150\n","60/60 [==============================] - 0s 7ms/step - loss: 697.1104 - mae: 697.1104 - val_loss: 694.8673 - val_mae: 694.8673\n","Epoch 63/150\n","60/60 [==============================] - 0s 6ms/step - loss: 692.4849 - mae: 692.4849 - val_loss: 690.5180 - val_mae: 690.5180\n","Epoch 64/150\n","60/60 [==============================] - 0s 6ms/step - loss: 687.7390 - mae: 687.7390 - val_loss: 684.5026 - val_mae: 684.5026\n","Epoch 65/150\n","60/60 [==============================] - 0s 6ms/step - loss: 683.0182 - mae: 683.0182 - val_loss: 679.6179 - val_mae: 679.6179\n","Epoch 66/150\n","60/60 [==============================] - 0s 7ms/step - loss: 678.2435 - mae: 678.2435 - val_loss: 676.0188 - val_mae: 676.0188\n","Epoch 67/150\n","60/60 [==============================] - 0s 6ms/step - loss: 673.3539 - mae: 673.3539 - val_loss: 671.6752 - val_mae: 671.6752\n","Epoch 68/150\n","60/60 [==============================] - 0s 6ms/step - loss: 668.5180 - mae: 668.5180 - val_loss: 666.1833 - val_mae: 666.1833\n","Epoch 69/150\n","60/60 [==============================] - 0s 6ms/step - loss: 663.5823 - mae: 663.5823 - val_loss: 660.4125 - val_mae: 660.4125\n","Epoch 70/150\n","60/60 [==============================] - 0s 6ms/step - loss: 658.6033 - mae: 658.6033 - val_loss: 655.8796 - val_mae: 655.8796\n","Epoch 71/150\n","60/60 [==============================] - 0s 6ms/step - loss: 653.4113 - mae: 653.4113 - val_loss: 649.3173 - val_mae: 649.3172\n","Epoch 72/150\n","60/60 [==============================] - 0s 6ms/step - loss: 648.3113 - mae: 648.3113 - val_loss: 642.7732 - val_mae: 642.7732\n","Epoch 73/150\n","60/60 [==============================] - 0s 6ms/step - loss: 643.2449 - mae: 643.2449 - val_loss: 639.4096 - val_mae: 639.4096\n","Epoch 74/150\n","60/60 [==============================] - 0s 8ms/step - loss: 637.8585 - mae: 637.8585 - val_loss: 633.6091 - val_mae: 633.6091\n","Epoch 75/150\n","60/60 [==============================] - 0s 8ms/step - loss: 632.7499 - mae: 632.7499 - val_loss: 625.9641 - val_mae: 625.9641\n","Epoch 76/150\n","60/60 [==============================] - 0s 7ms/step - loss: 627.2971 - mae: 627.2971 - val_loss: 619.4518 - val_mae: 619.4518\n","Epoch 77/150\n","60/60 [==============================] - 0s 7ms/step - loss: 621.7540 - mae: 621.7540 - val_loss: 615.9752 - val_mae: 615.9752\n","Epoch 78/150\n","60/60 [==============================] - 0s 8ms/step - loss: 616.1588 - mae: 616.1588 - val_loss: 608.4884 - val_mae: 608.4884\n","Epoch 79/150\n","60/60 [==============================] - 0s 7ms/step - loss: 610.6620 - mae: 610.6620 - val_loss: 603.1558 - val_mae: 603.1558\n","Epoch 80/150\n","60/60 [==============================] - 0s 6ms/step - loss: 604.6513 - mae: 604.6513 - val_loss: 592.1271 - val_mae: 592.1271\n","Epoch 81/150\n","60/60 [==============================] - 0s 8ms/step - loss: 599.1293 - mae: 599.1293 - val_loss: 584.8988 - val_mae: 584.8988\n","Epoch 82/150\n","60/60 [==============================] - 0s 6ms/step - loss: 593.3543 - mae: 593.3543 - val_loss: 582.8646 - val_mae: 582.8646\n","Epoch 83/150\n","60/60 [==============================] - 0s 7ms/step - loss: 587.2825 - mae: 587.2825 - val_loss: 572.2427 - val_mae: 572.2427\n","Epoch 84/150\n","60/60 [==============================] - 0s 7ms/step - loss: 581.5784 - mae: 581.5784 - val_loss: 568.1120 - val_mae: 568.1120\n","Epoch 85/150\n","60/60 [==============================] - 0s 7ms/step - loss: 575.4706 - mae: 575.4706 - val_loss: 561.0405 - val_mae: 561.0405\n","Epoch 86/150\n","60/60 [==============================] - 0s 7ms/step - loss: 569.2059 - mae: 569.2059 - val_loss: 554.5152 - val_mae: 554.5152\n","Epoch 87/150\n","60/60 [==============================] - 0s 7ms/step - loss: 562.8630 - mae: 562.8630 - val_loss: 547.6292 - val_mae: 547.6292\n","Epoch 88/150\n","60/60 [==============================] - 0s 6ms/step - loss: 557.0415 - mae: 557.0415 - val_loss: 542.9064 - val_mae: 542.9064\n","Epoch 89/150\n","60/60 [==============================] - 0s 7ms/step - loss: 550.4285 - mae: 550.4285 - val_loss: 536.3834 - val_mae: 536.3834\n","Epoch 90/150\n","60/60 [==============================] - 0s 6ms/step - loss: 544.1241 - mae: 544.1241 - val_loss: 529.5845 - val_mae: 529.5845\n","Epoch 91/150\n","60/60 [==============================] - 0s 7ms/step - loss: 537.3429 - mae: 537.3429 - val_loss: 515.8632 - val_mae: 515.8632\n","Epoch 92/150\n","60/60 [==============================] - 0s 7ms/step - loss: 530.3505 - mae: 530.3505 - val_loss: 509.1237 - val_mae: 509.1237\n","Epoch 93/150\n","60/60 [==============================] - 0s 7ms/step - loss: 524.2293 - mae: 524.2293 - val_loss: 506.6934 - val_mae: 506.6934\n","Epoch 94/150\n","60/60 [==============================] - 0s 7ms/step - loss: 517.6583 - mae: 517.6583 - val_loss: 501.5392 - val_mae: 501.5392\n","Epoch 95/150\n","60/60 [==============================] - 1s 12ms/step - loss: 510.9948 - mae: 510.9948 - val_loss: 495.8561 - val_mae: 495.8561\n","Epoch 96/150\n","60/60 [==============================] - 0s 6ms/step - loss: 504.0216 - mae: 504.0216 - val_loss: 492.7477 - val_mae: 492.7477\n","Epoch 97/150\n","60/60 [==============================] - 1s 15ms/step - loss: 497.3292 - mae: 497.3292 - val_loss: 480.3073 - val_mae: 480.3073\n","Epoch 98/150\n","60/60 [==============================] - 0s 7ms/step - loss: 491.7708 - mae: 491.7708 - val_loss: 476.5680 - val_mae: 476.5680\n","Epoch 99/150\n","60/60 [==============================] - 0s 7ms/step - loss: 484.3076 - mae: 484.3076 - val_loss: 469.3717 - val_mae: 469.3717\n","Epoch 100/150\n","60/60 [==============================] - 0s 8ms/step - loss: 477.4892 - mae: 477.4892 - val_loss: 459.9596 - val_mae: 459.9596\n","Epoch 101/150\n","60/60 [==============================] - 0s 7ms/step - loss: 471.0769 - mae: 471.0769 - val_loss: 453.7814 - val_mae: 453.7814\n","Epoch 102/150\n","60/60 [==============================] - 0s 7ms/step - loss: 463.8613 - mae: 463.8613 - val_loss: 444.0909 - val_mae: 444.0908\n","Epoch 103/150\n","60/60 [==============================] - 0s 8ms/step - loss: 458.1769 - mae: 458.1769 - val_loss: 434.5724 - val_mae: 434.5724\n","Epoch 104/150\n","60/60 [==============================] - 0s 7ms/step - loss: 450.7433 - mae: 450.7433 - val_loss: 423.9427 - val_mae: 423.9427\n","Epoch 105/150\n","60/60 [==============================] - 0s 7ms/step - loss: 443.2677 - mae: 443.2677 - val_loss: 414.8995 - val_mae: 414.8995\n","Epoch 106/150\n","60/60 [==============================] - 0s 8ms/step - loss: 437.1324 - mae: 437.1324 - val_loss: 409.0196 - val_mae: 409.0196\n","Epoch 107/150\n","60/60 [==============================] - 0s 7ms/step - loss: 431.0823 - mae: 431.0823 - val_loss: 404.8789 - val_mae: 404.8789\n","Epoch 108/150\n","60/60 [==============================] - 0s 7ms/step - loss: 424.2466 - mae: 424.2466 - val_loss: 398.2274 - val_mae: 398.2274\n","Epoch 109/150\n","60/60 [==============================] - 0s 7ms/step - loss: 417.4198 - mae: 417.4198 - val_loss: 396.3271 - val_mae: 396.3271\n","Epoch 110/150\n","60/60 [==============================] - 0s 7ms/step - loss: 411.5667 - mae: 411.5667 - val_loss: 390.9966 - val_mae: 390.9966\n","Epoch 111/150\n","60/60 [==============================] - 0s 7ms/step - loss: 404.4802 - mae: 404.4802 - val_loss: 377.1577 - val_mae: 377.1577\n","Epoch 112/150\n","60/60 [==============================] - 0s 7ms/step - loss: 398.8817 - mae: 398.8817 - val_loss: 371.0155 - val_mae: 371.0155\n","Epoch 113/150\n","60/60 [==============================] - 0s 7ms/step - loss: 392.4453 - mae: 392.4453 - val_loss: 363.7385 - val_mae: 363.7385\n","Epoch 114/150\n","60/60 [==============================] - 0s 7ms/step - loss: 387.7375 - mae: 387.7375 - val_loss: 362.8742 - val_mae: 362.8742\n","Epoch 115/150\n","60/60 [==============================] - 0s 7ms/step - loss: 381.2059 - mae: 381.2059 - val_loss: 360.6761 - val_mae: 360.6760\n","Epoch 116/150\n","60/60 [==============================] - 0s 6ms/step - loss: 376.1617 - mae: 376.1617 - val_loss: 347.8157 - val_mae: 347.8157\n","Epoch 117/150\n","60/60 [==============================] - 0s 7ms/step - loss: 373.6648 - mae: 373.6647 - val_loss: 345.8959 - val_mae: 345.8959\n","Epoch 118/150\n","60/60 [==============================] - 0s 7ms/step - loss: 367.0602 - mae: 367.0602 - val_loss: 337.0269 - val_mae: 337.0269\n","Epoch 119/150\n","60/60 [==============================] - 0s 7ms/step - loss: 360.0233 - mae: 360.0233 - val_loss: 335.9235 - val_mae: 335.9235\n","Epoch 120/150\n","60/60 [==============================] - 0s 7ms/step - loss: 357.7137 - mae: 357.7137 - val_loss: 334.7731 - val_mae: 334.7731\n","Epoch 121/150\n","60/60 [==============================] - 0s 7ms/step - loss: 352.2378 - mae: 352.2378 - val_loss: 324.8483 - val_mae: 324.8483\n","Epoch 122/150\n","60/60 [==============================] - 0s 7ms/step - loss: 348.2360 - mae: 348.2360 - val_loss: 317.9243 - val_mae: 317.9243\n","Epoch 123/150\n","60/60 [==============================] - 0s 7ms/step - loss: 348.1749 - mae: 348.1749 - val_loss: 312.2815 - val_mae: 312.2816\n","Epoch 124/150\n","60/60 [==============================] - 0s 7ms/step - loss: 341.6414 - mae: 341.6414 - val_loss: 309.7933 - val_mae: 309.7933\n","Epoch 125/150\n","60/60 [==============================] - 0s 8ms/step - loss: 340.8932 - mae: 340.8932 - val_loss: 312.2099 - val_mae: 312.2099\n","Epoch 126/150\n","60/60 [==============================] - 0s 7ms/step - loss: 334.6703 - mae: 334.6703 - val_loss: 307.3904 - val_mae: 307.3904\n","Epoch 127/150\n","60/60 [==============================] - 0s 7ms/step - loss: 334.1276 - mae: 334.1276 - val_loss: 301.7458 - val_mae: 301.7458\n","Epoch 128/150\n","60/60 [==============================] - 0s 6ms/step - loss: 328.6577 - mae: 328.6577 - val_loss: 300.0855 - val_mae: 300.0855\n","Epoch 129/150\n","60/60 [==============================] - 0s 7ms/step - loss: 329.7185 - mae: 329.7185 - val_loss: 300.4323 - val_mae: 300.4323\n","Epoch 130/150\n","60/60 [==============================] - 0s 8ms/step - loss: 325.7556 - mae: 325.7556 - val_loss: 299.2675 - val_mae: 299.2675\n","Epoch 131/150\n","60/60 [==============================] - 0s 7ms/step - loss: 328.5556 - mae: 328.5556 - val_loss: 296.9884 - val_mae: 296.9884\n","Epoch 132/150\n","60/60 [==============================] - 0s 8ms/step - loss: 323.8748 - mae: 323.8748 - val_loss: 294.9390 - val_mae: 294.9390\n","Epoch 133/150\n","60/60 [==============================] - 0s 7ms/step - loss: 322.8135 - mae: 322.8135 - val_loss: 294.2901 - val_mae: 294.2901\n","Epoch 134/150\n","60/60 [==============================] - 0s 6ms/step - loss: 323.0597 - mae: 323.0597 - val_loss: 296.1533 - val_mae: 296.1533\n","Epoch 135/150\n","60/60 [==============================] - 0s 7ms/step - loss: 317.8482 - mae: 317.8482 - val_loss: 292.0478 - val_mae: 292.0478\n","Epoch 136/150\n","60/60 [==============================] - 0s 6ms/step - loss: 318.2293 - mae: 318.2293 - val_loss: 291.0588 - val_mae: 291.0588\n","Epoch 137/150\n","60/60 [==============================] - 0s 6ms/step - loss: 320.3866 - mae: 320.3866 - val_loss: 293.1342 - val_mae: 293.1342\n","Epoch 138/150\n","60/60 [==============================] - 0s 6ms/step - loss: 320.7949 - mae: 320.7949 - val_loss: 291.0856 - val_mae: 291.0856\n","Epoch 139/150\n","60/60 [==============================] - 0s 6ms/step - loss: 317.6426 - mae: 317.6426 - val_loss: 291.6897 - val_mae: 291.6897\n","Epoch 140/150\n","60/60 [==============================] - 0s 6ms/step - loss: 319.6356 - mae: 319.6356 - val_loss: 290.9664 - val_mae: 290.9664\n","Epoch 141/150\n","60/60 [==============================] - 0s 6ms/step - loss: 317.4088 - mae: 317.4088 - val_loss: 289.5311 - val_mae: 289.5311\n","Epoch 142/150\n","60/60 [==============================] - 0s 7ms/step - loss: 317.9704 - mae: 317.9704 - val_loss: 290.1108 - val_mae: 290.1108\n","Epoch 143/150\n","60/60 [==============================] - 0s 7ms/step - loss: 315.8133 - mae: 315.8133 - val_loss: 289.4815 - val_mae: 289.4815\n","Epoch 144/150\n","60/60 [==============================] - 0s 7ms/step - loss: 317.7210 - mae: 317.7210 - val_loss: 291.2920 - val_mae: 291.2920\n","Epoch 145/150\n","60/60 [==============================] - 0s 8ms/step - loss: 317.3945 - mae: 317.3945 - val_loss: 287.9130 - val_mae: 287.9130\n","Epoch 146/150\n","60/60 [==============================] - 0s 7ms/step - loss: 319.1809 - mae: 319.1809 - val_loss: 289.2591 - val_mae: 289.2591\n","Epoch 147/150\n","60/60 [==============================] - 0s 7ms/step - loss: 314.5887 - mae: 314.5887 - val_loss: 289.3835 - val_mae: 289.3835\n","Epoch 148/150\n","60/60 [==============================] - 0s 7ms/step - loss: 313.8328 - mae: 313.8328 - val_loss: 288.2489 - val_mae: 288.2489\n","Epoch 149/150\n","60/60 [==============================] - 0s 8ms/step - loss: 315.3079 - mae: 315.3079 - val_loss: 290.9856 - val_mae: 290.9856\n","Epoch 150/150\n","60/60 [==============================] - 0s 7ms/step - loss: 318.4290 - mae: 318.4290 - val_loss: 288.2677 - val_mae: 288.2677\n","Processing fold #: 1\n","Epoch 1/150\n","60/60 [==============================] - 2s 10ms/step - loss: 849.2782 - mae: 849.2782 - val_loss: 849.6335 - val_mae: 849.6335\n","Epoch 2/150\n","60/60 [==============================] - 0s 7ms/step - loss: 848.8668 - mae: 848.8668 - val_loss: 849.1243 - val_mae: 849.1243\n","Epoch 3/150\n","60/60 [==============================] - 0s 8ms/step - loss: 848.3846 - mae: 848.3846 - val_loss: 848.6498 - val_mae: 848.6498\n","Epoch 4/150\n","60/60 [==============================] - 0s 8ms/step - loss: 847.8279 - mae: 847.8279 - val_loss: 848.1556 - val_mae: 848.1556\n","Epoch 5/150\n","60/60 [==============================] - 0s 8ms/step - loss: 847.1972 - mae: 847.1972 - val_loss: 847.6781 - val_mae: 847.6781\n","Epoch 6/150\n","60/60 [==============================] - 0s 7ms/step - loss: 846.4929 - mae: 846.4929 - val_loss: 847.1356 - val_mae: 847.1356\n","Epoch 7/150\n","60/60 [==============================] - 0s 7ms/step - loss: 845.7150 - mae: 845.7150 - val_loss: 846.2939 - val_mae: 846.2939\n","Epoch 8/150\n","60/60 [==============================] - 0s 7ms/step - loss: 844.8641 - mae: 844.8641 - val_loss: 845.5108 - val_mae: 845.5108\n","Epoch 9/150\n","60/60 [==============================] - 0s 8ms/step - loss: 843.9403 - mae: 843.9403 - val_loss: 844.6851 - val_mae: 844.6851\n","Epoch 10/150\n","60/60 [==============================] - 0s 8ms/step - loss: 842.9432 - mae: 842.9432 - val_loss: 843.6618 - val_mae: 843.6618\n","Epoch 11/150\n","60/60 [==============================] - 0s 7ms/step - loss: 841.8735 - mae: 841.8735 - val_loss: 842.6118 - val_mae: 842.6119\n","Epoch 12/150\n","60/60 [==============================] - 0s 7ms/step - loss: 840.7305 - mae: 840.7305 - val_loss: 841.4915 - val_mae: 841.4915\n","Epoch 13/150\n","60/60 [==============================] - 0s 6ms/step - loss: 839.5153 - mae: 839.5153 - val_loss: 840.3248 - val_mae: 840.3248\n","Epoch 14/150\n","60/60 [==============================] - 0s 6ms/step - loss: 838.2274 - mae: 838.2274 - val_loss: 838.9916 - val_mae: 838.9916\n","Epoch 15/150\n","60/60 [==============================] - 0s 7ms/step - loss: 836.8665 - mae: 836.8665 - val_loss: 837.7337 - val_mae: 837.7337\n","Epoch 16/150\n","60/60 [==============================] - 0s 6ms/step - loss: 835.4332 - mae: 835.4332 - val_loss: 836.2960 - val_mae: 836.2960\n","Epoch 17/150\n","60/60 [==============================] - 0s 6ms/step - loss: 833.9272 - mae: 833.9272 - val_loss: 834.6599 - val_mae: 834.6599\n","Epoch 18/150\n","60/60 [==============================] - 0s 6ms/step - loss: 832.3487 - mae: 832.3487 - val_loss: 833.0975 - val_mae: 833.0976\n","Epoch 19/150\n","60/60 [==============================] - 0s 7ms/step - loss: 830.6978 - mae: 830.6978 - val_loss: 831.3690 - val_mae: 831.3690\n","Epoch 20/150\n","60/60 [==============================] - 0s 6ms/step - loss: 828.9742 - mae: 828.9742 - val_loss: 829.8002 - val_mae: 829.8002\n","Epoch 21/150\n","60/60 [==============================] - 0s 7ms/step - loss: 827.1783 - mae: 827.1783 - val_loss: 827.9693 - val_mae: 827.9693\n","Epoch 22/150\n","60/60 [==============================] - 0s 7ms/step - loss: 825.3101 - mae: 825.3101 - val_loss: 826.2930 - val_mae: 826.2930\n","Epoch 23/150\n","60/60 [==============================] - 0s 7ms/step - loss: 823.3692 - mae: 823.3692 - val_loss: 824.2827 - val_mae: 824.2827\n","Epoch 24/150\n","60/60 [==============================] - 0s 8ms/step - loss: 821.3558 - mae: 821.3558 - val_loss: 822.4756 - val_mae: 822.4756\n","Epoch 25/150\n","60/60 [==============================] - 0s 7ms/step - loss: 819.2701 - mae: 819.2701 - val_loss: 820.2468 - val_mae: 820.2468\n","Epoch 26/150\n","60/60 [==============================] - 0s 8ms/step - loss: 817.1120 - mae: 817.1120 - val_loss: 818.1545 - val_mae: 818.1545\n","Epoch 27/150\n","60/60 [==============================] - 0s 8ms/step - loss: 814.8816 - mae: 814.8816 - val_loss: 815.8316 - val_mae: 815.8316\n","Epoch 28/150\n","60/60 [==============================] - 0s 7ms/step - loss: 812.5789 - mae: 812.5789 - val_loss: 813.6250 - val_mae: 813.6250\n","Epoch 29/150\n","60/60 [==============================] - 0s 7ms/step - loss: 810.2038 - mae: 810.2037 - val_loss: 811.4244 - val_mae: 811.4244\n","Epoch 30/150\n","60/60 [==============================] - 0s 7ms/step - loss: 807.7562 - mae: 807.7562 - val_loss: 808.5042 - val_mae: 808.5042\n","Epoch 31/150\n","60/60 [==============================] - 0s 7ms/step - loss: 805.2363 - mae: 805.2363 - val_loss: 806.2054 - val_mae: 806.2054\n","Epoch 32/150\n","60/60 [==============================] - 0s 7ms/step - loss: 802.6444 - mae: 802.6444 - val_loss: 803.4790 - val_mae: 803.4790\n","Epoch 33/150\n","60/60 [==============================] - 0s 8ms/step - loss: 799.9800 - mae: 799.9800 - val_loss: 800.6978 - val_mae: 800.6978\n","Epoch 34/150\n","60/60 [==============================] - 0s 7ms/step - loss: 797.2433 - mae: 797.2433 - val_loss: 798.1092 - val_mae: 798.1092\n","Epoch 35/150\n","60/60 [==============================] - 0s 8ms/step - loss: 794.4342 - mae: 794.4343 - val_loss: 794.9244 - val_mae: 794.9244\n","Epoch 36/150\n","60/60 [==============================] - 0s 7ms/step - loss: 791.5543 - mae: 791.5543 - val_loss: 791.7663 - val_mae: 791.7663\n","Epoch 37/150\n","60/60 [==============================] - 0s 6ms/step - loss: 788.6009 - mae: 788.6009 - val_loss: 788.1486 - val_mae: 788.1486\n","Epoch 38/150\n","60/60 [==============================] - 0s 7ms/step - loss: 785.5767 - mae: 785.5767 - val_loss: 784.3406 - val_mae: 784.3406\n","Epoch 39/150\n","60/60 [==============================] - 0s 6ms/step - loss: 782.4860 - mae: 782.4860 - val_loss: 780.8984 - val_mae: 780.8984\n","Epoch 40/150\n","60/60 [==============================] - 0s 7ms/step - loss: 779.3199 - mae: 779.3199 - val_loss: 777.1763 - val_mae: 777.1763\n","Epoch 41/150\n","60/60 [==============================] - 0s 7ms/step - loss: 776.0884 - mae: 776.0884 - val_loss: 773.5103 - val_mae: 773.5104\n","Epoch 42/150\n","60/60 [==============================] - 0s 7ms/step - loss: 772.7763 - mae: 772.7763 - val_loss: 769.2823 - val_mae: 769.2823\n","Epoch 43/150\n","60/60 [==============================] - 0s 7ms/step - loss: 769.4023 - mae: 769.4023 - val_loss: 765.4081 - val_mae: 765.4080\n","Epoch 44/150\n","60/60 [==============================] - 0s 6ms/step - loss: 765.9498 - mae: 765.9500 - val_loss: 762.9428 - val_mae: 762.9428\n","Epoch 45/150\n","60/60 [==============================] - 0s 6ms/step - loss: 762.4388 - mae: 762.4388 - val_loss: 759.1984 - val_mae: 759.1984\n","Epoch 46/150\n","60/60 [==============================] - 0s 7ms/step - loss: 758.8572 - mae: 758.8572 - val_loss: 755.1501 - val_mae: 755.1501\n","Epoch 47/150\n","60/60 [==============================] - 0s 6ms/step - loss: 755.2083 - mae: 755.2083 - val_loss: 751.2292 - val_mae: 751.2292\n","Epoch 48/150\n","60/60 [==============================] - 0s 7ms/step - loss: 751.4935 - mae: 751.4935 - val_loss: 748.6633 - val_mae: 748.6633\n","Epoch 49/150\n","60/60 [==============================] - 0s 7ms/step - loss: 747.6947 - mae: 747.6947 - val_loss: 744.5713 - val_mae: 744.5712\n","Epoch 50/150\n","60/60 [==============================] - 0s 7ms/step - loss: 743.8447 - mae: 743.8447 - val_loss: 741.4401 - val_mae: 741.4401\n","Epoch 51/150\n","60/60 [==============================] - 0s 7ms/step - loss: 739.9316 - mae: 739.9316 - val_loss: 737.2959 - val_mae: 737.2959\n","Epoch 52/150\n","60/60 [==============================] - 0s 7ms/step - loss: 735.9396 - mae: 735.9396 - val_loss: 733.5760 - val_mae: 733.5760\n","Epoch 53/150\n","60/60 [==============================] - 0s 8ms/step - loss: 731.9120 - mae: 731.9120 - val_loss: 728.6631 - val_mae: 728.6631\n","Epoch 54/150\n","60/60 [==============================] - 0s 7ms/step - loss: 727.7918 - mae: 727.7918 - val_loss: 724.2749 - val_mae: 724.2749\n","Epoch 55/150\n","60/60 [==============================] - 0s 8ms/step - loss: 723.5916 - mae: 723.5916 - val_loss: 720.5912 - val_mae: 720.5912\n","Epoch 56/150\n","60/60 [==============================] - 0s 7ms/step - loss: 719.3654 - mae: 719.3654 - val_loss: 717.3762 - val_mae: 717.3762\n","Epoch 57/150\n","60/60 [==============================] - 0s 8ms/step - loss: 715.0745 - mae: 715.0745 - val_loss: 712.7778 - val_mae: 712.7778\n","Epoch 58/150\n","60/60 [==============================] - 0s 7ms/step - loss: 710.7086 - mae: 710.7086 - val_loss: 708.8116 - val_mae: 708.8116\n","Epoch 59/150\n","60/60 [==============================] - 0s 7ms/step - loss: 706.2922 - mae: 706.2922 - val_loss: 703.5956 - val_mae: 703.5956\n","Epoch 60/150\n","60/60 [==============================] - 0s 7ms/step - loss: 701.8157 - mae: 701.8157 - val_loss: 698.5956 - val_mae: 698.5956\n","Epoch 61/150\n","60/60 [==============================] - 0s 8ms/step - loss: 697.2975 - mae: 697.2975 - val_loss: 695.0019 - val_mae: 695.0019\n","Epoch 62/150\n","60/60 [==============================] - 0s 8ms/step - loss: 692.7150 - mae: 692.7150 - val_loss: 690.3829 - val_mae: 690.3829\n","Epoch 63/150\n","60/60 [==============================] - 0s 7ms/step - loss: 688.0892 - mae: 688.0892 - val_loss: 685.2924 - val_mae: 685.2924\n","Epoch 64/150\n","60/60 [==============================] - 0s 7ms/step - loss: 683.4137 - mae: 683.4137 - val_loss: 680.3085 - val_mae: 680.3085\n","Epoch 65/150\n","60/60 [==============================] - 0s 8ms/step - loss: 678.7379 - mae: 678.7379 - val_loss: 675.1010 - val_mae: 675.1010\n","Epoch 66/150\n","60/60 [==============================] - 0s 7ms/step - loss: 673.8904 - mae: 673.8904 - val_loss: 671.6069 - val_mae: 671.6069\n","Epoch 67/150\n","60/60 [==============================] - 0s 8ms/step - loss: 669.2139 - mae: 669.2139 - val_loss: 666.2917 - val_mae: 666.2917\n","Epoch 68/150\n","60/60 [==============================] - 0s 7ms/step - loss: 664.3360 - mae: 664.3360 - val_loss: 661.5737 - val_mae: 661.5737\n","Epoch 69/150\n","60/60 [==============================] - 0s 7ms/step - loss: 659.4969 - mae: 659.4969 - val_loss: 656.5232 - val_mae: 656.5232\n","Epoch 70/150\n","60/60 [==============================] - 0s 7ms/step - loss: 654.5684 - mae: 654.5684 - val_loss: 651.5030 - val_mae: 651.5030\n","Epoch 71/150\n","60/60 [==============================] - 0s 7ms/step - loss: 649.7134 - mae: 649.7134 - val_loss: 646.2601 - val_mae: 646.2601\n","Epoch 72/150\n","60/60 [==============================] - 0s 7ms/step - loss: 644.7531 - mae: 644.7531 - val_loss: 641.3810 - val_mae: 641.3809\n","Epoch 73/150\n","60/60 [==============================] - 0s 7ms/step - loss: 639.6680 - mae: 639.6680 - val_loss: 634.6508 - val_mae: 634.6508\n","Epoch 74/150\n","60/60 [==============================] - 0s 7ms/step - loss: 634.5825 - mae: 634.5825 - val_loss: 629.4435 - val_mae: 629.4435\n","Epoch 75/150\n","60/60 [==============================] - 0s 7ms/step - loss: 629.2409 - mae: 629.2409 - val_loss: 624.8546 - val_mae: 624.8546\n","Epoch 76/150\n","60/60 [==============================] - 0s 7ms/step - loss: 623.9785 - mae: 623.9785 - val_loss: 619.5968 - val_mae: 619.5968\n","Epoch 77/150\n","60/60 [==============================] - 0s 7ms/step - loss: 618.5925 - mae: 618.5925 - val_loss: 613.6993 - val_mae: 613.6993\n","Epoch 78/150\n","60/60 [==============================] - 0s 7ms/step - loss: 613.2498 - mae: 613.2498 - val_loss: 606.9682 - val_mae: 606.9682\n","Epoch 79/150\n","60/60 [==============================] - 0s 8ms/step - loss: 607.6464 - mae: 607.6464 - val_loss: 600.3138 - val_mae: 600.3138\n","Epoch 80/150\n","60/60 [==============================] - 0s 7ms/step - loss: 602.0178 - mae: 602.0178 - val_loss: 594.5834 - val_mae: 594.5834\n","Epoch 81/150\n","60/60 [==============================] - 0s 7ms/step - loss: 596.0269 - mae: 596.0269 - val_loss: 588.0740 - val_mae: 588.0740\n","Epoch 82/150\n","60/60 [==============================] - 0s 7ms/step - loss: 590.0820 - mae: 590.0820 - val_loss: 579.9638 - val_mae: 579.9638\n","Epoch 83/150\n","60/60 [==============================] - 0s 7ms/step - loss: 583.6809 - mae: 583.6809 - val_loss: 574.2733 - val_mae: 574.2733\n","Epoch 84/150\n","60/60 [==============================] - 0s 7ms/step - loss: 577.8366 - mae: 577.8366 - val_loss: 568.3811 - val_mae: 568.3811\n","Epoch 85/150\n","60/60 [==============================] - 0s 7ms/step - loss: 572.1982 - mae: 572.1982 - val_loss: 562.4182 - val_mae: 562.4182\n","Epoch 86/150\n","60/60 [==============================] - 0s 7ms/step - loss: 565.9879 - mae: 565.9879 - val_loss: 559.4153 - val_mae: 559.4153\n","Epoch 87/150\n","60/60 [==============================] - 0s 7ms/step - loss: 560.0832 - mae: 560.0832 - val_loss: 555.4656 - val_mae: 555.4656\n","Epoch 88/150\n","60/60 [==============================] - 0s 7ms/step - loss: 554.0801 - mae: 554.0801 - val_loss: 550.7565 - val_mae: 550.7565\n","Epoch 89/150\n","60/60 [==============================] - 0s 8ms/step - loss: 547.8060 - mae: 547.8060 - val_loss: 543.4019 - val_mae: 543.4019\n","Epoch 90/150\n","60/60 [==============================] - 0s 7ms/step - loss: 541.6012 - mae: 541.6012 - val_loss: 538.1738 - val_mae: 538.1738\n","Epoch 91/150\n","60/60 [==============================] - 0s 7ms/step - loss: 536.0977 - mae: 536.0977 - val_loss: 536.8939 - val_mae: 536.8939\n","Epoch 92/150\n","60/60 [==============================] - 0s 7ms/step - loss: 529.8787 - mae: 529.8787 - val_loss: 531.9116 - val_mae: 531.9116\n","Epoch 93/150\n","60/60 [==============================] - 0s 7ms/step - loss: 523.6188 - mae: 523.6188 - val_loss: 523.7964 - val_mae: 523.7964\n","Epoch 94/150\n","60/60 [==============================] - 0s 7ms/step - loss: 517.9843 - mae: 517.9843 - val_loss: 518.5815 - val_mae: 518.5815\n","Epoch 95/150\n","60/60 [==============================] - 0s 7ms/step - loss: 512.1173 - mae: 512.1173 - val_loss: 515.7207 - val_mae: 515.7207\n","Epoch 96/150\n","60/60 [==============================] - 0s 6ms/step - loss: 505.6548 - mae: 505.6548 - val_loss: 507.9769 - val_mae: 507.9768\n","Epoch 97/150\n","60/60 [==============================] - 0s 6ms/step - loss: 499.9441 - mae: 499.9441 - val_loss: 501.7195 - val_mae: 501.7195\n","Epoch 98/150\n","60/60 [==============================] - 0s 6ms/step - loss: 494.2834 - mae: 494.2834 - val_loss: 497.8899 - val_mae: 497.8899\n","Epoch 99/150\n","60/60 [==============================] - 0s 6ms/step - loss: 487.7921 - mae: 487.7921 - val_loss: 488.0578 - val_mae: 488.0578\n","Epoch 100/150\n","60/60 [==============================] - 0s 7ms/step - loss: 482.0818 - mae: 482.0818 - val_loss: 484.4845 - val_mae: 484.4845\n","Epoch 101/150\n","60/60 [==============================] - 0s 6ms/step - loss: 475.5089 - mae: 475.5089 - val_loss: 476.1503 - val_mae: 476.1503\n","Epoch 102/150\n","60/60 [==============================] - 0s 6ms/step - loss: 470.6421 - mae: 470.6421 - val_loss: 470.0274 - val_mae: 470.0274\n","Epoch 103/150\n","60/60 [==============================] - 0s 7ms/step - loss: 465.1171 - mae: 465.1171 - val_loss: 465.2424 - val_mae: 465.2424\n","Epoch 104/150\n","60/60 [==============================] - 0s 6ms/step - loss: 457.7539 - mae: 457.7539 - val_loss: 458.1063 - val_mae: 458.1063\n","Epoch 105/150\n","60/60 [==============================] - 0s 6ms/step - loss: 453.5170 - mae: 453.5170 - val_loss: 454.2933 - val_mae: 454.2933\n","Epoch 106/150\n","60/60 [==============================] - 0s 7ms/step - loss: 446.0232 - mae: 446.0232 - val_loss: 448.2372 - val_mae: 448.2372\n","Epoch 107/150\n","60/60 [==============================] - 0s 7ms/step - loss: 441.9836 - mae: 441.9836 - val_loss: 444.9323 - val_mae: 444.9323\n","Epoch 108/150\n","60/60 [==============================] - 0s 7ms/step - loss: 436.5130 - mae: 436.5130 - val_loss: 440.3210 - val_mae: 440.3210\n","Epoch 109/150\n","60/60 [==============================] - 0s 6ms/step - loss: 430.8391 - mae: 430.8391 - val_loss: 435.1942 - val_mae: 435.1942\n","Epoch 110/150\n","60/60 [==============================] - 0s 7ms/step - loss: 427.8607 - mae: 427.8607 - val_loss: 432.1137 - val_mae: 432.1137\n","Epoch 111/150\n","60/60 [==============================] - 0s 7ms/step - loss: 422.0348 - mae: 422.0348 - val_loss: 425.1206 - val_mae: 425.1206\n","Epoch 112/150\n","60/60 [==============================] - 0s 7ms/step - loss: 418.0202 - mae: 418.0202 - val_loss: 420.6745 - val_mae: 420.6745\n","Epoch 113/150\n","60/60 [==============================] - 0s 8ms/step - loss: 411.8132 - mae: 411.8132 - val_loss: 416.8082 - val_mae: 416.8082\n","Epoch 114/150\n","60/60 [==============================] - 0s 7ms/step - loss: 407.0973 - mae: 407.0973 - val_loss: 410.3976 - val_mae: 410.3976\n","Epoch 115/150\n","60/60 [==============================] - 0s 6ms/step - loss: 404.8941 - mae: 404.8942 - val_loss: 407.1522 - val_mae: 407.1522\n","Epoch 116/150\n","60/60 [==============================] - 0s 6ms/step - loss: 398.1184 - mae: 398.1184 - val_loss: 402.2025 - val_mae: 402.2025\n","Epoch 117/150\n","60/60 [==============================] - 0s 7ms/step - loss: 393.4893 - mae: 393.4893 - val_loss: 398.2174 - val_mae: 398.2174\n","Epoch 118/150\n","60/60 [==============================] - 0s 7ms/step - loss: 391.4064 - mae: 391.4064 - val_loss: 398.3456 - val_mae: 398.3456\n","Epoch 119/150\n","60/60 [==============================] - 0s 8ms/step - loss: 387.8262 - mae: 387.8262 - val_loss: 391.4202 - val_mae: 391.4202\n","Epoch 120/150\n","60/60 [==============================] - 0s 7ms/step - loss: 384.7256 - mae: 384.7256 - val_loss: 388.2287 - val_mae: 388.2287\n","Epoch 121/150\n","60/60 [==============================] - 0s 8ms/step - loss: 379.0063 - mae: 379.0063 - val_loss: 384.1595 - val_mae: 384.1595\n","Epoch 122/150\n","60/60 [==============================] - 1s 9ms/step - loss: 375.4746 - mae: 375.4746 - val_loss: 378.8825 - val_mae: 378.8825\n","Epoch 123/150\n","60/60 [==============================] - 0s 8ms/step - loss: 370.0816 - mae: 370.0816 - val_loss: 372.3081 - val_mae: 372.3081\n","Epoch 124/150\n","60/60 [==============================] - 0s 8ms/step - loss: 365.0045 - mae: 365.0045 - val_loss: 367.7833 - val_mae: 367.7833\n","Epoch 125/150\n","60/60 [==============================] - 0s 8ms/step - loss: 358.3516 - mae: 358.3516 - val_loss: 360.7005 - val_mae: 360.7005\n","Epoch 126/150\n","60/60 [==============================] - 1s 8ms/step - loss: 354.3483 - mae: 354.3483 - val_loss: 354.9045 - val_mae: 354.9045\n","Epoch 127/150\n","60/60 [==============================] - 0s 8ms/step - loss: 346.2575 - mae: 346.2575 - val_loss: 343.6326 - val_mae: 343.6326\n","Epoch 128/150\n","60/60 [==============================] - 0s 8ms/step - loss: 339.2881 - mae: 339.2881 - val_loss: 333.5429 - val_mae: 333.5429\n","Epoch 129/150\n","60/60 [==============================] - 0s 7ms/step - loss: 332.5903 - mae: 332.5903 - val_loss: 324.8080 - val_mae: 324.8080\n","Epoch 130/150\n","60/60 [==============================] - 0s 7ms/step - loss: 328.5414 - mae: 328.5414 - val_loss: 320.4175 - val_mae: 320.4175\n","Epoch 131/150\n","60/60 [==============================] - 0s 7ms/step - loss: 324.4189 - mae: 324.4189 - val_loss: 315.1707 - val_mae: 315.1707\n","Epoch 132/150\n","60/60 [==============================] - 0s 8ms/step - loss: 321.8517 - mae: 321.8517 - val_loss: 313.0627 - val_mae: 313.0627\n","Epoch 133/150\n","60/60 [==============================] - 0s 7ms/step - loss: 318.4545 - mae: 318.4545 - val_loss: 309.0241 - val_mae: 309.0241\n","Epoch 134/150\n","60/60 [==============================] - 0s 8ms/step - loss: 315.3463 - mae: 315.3463 - val_loss: 306.8370 - val_mae: 306.8370\n","Epoch 135/150\n","60/60 [==============================] - 0s 8ms/step - loss: 315.2663 - mae: 315.2663 - val_loss: 305.9460 - val_mae: 305.9460\n","Epoch 136/150\n","60/60 [==============================] - 0s 8ms/step - loss: 315.0388 - mae: 315.0388 - val_loss: 304.3651 - val_mae: 304.3651\n","Epoch 137/150\n","60/60 [==============================] - 0s 8ms/step - loss: 314.4570 - mae: 314.4570 - val_loss: 302.5352 - val_mae: 302.5352\n","Epoch 138/150\n","60/60 [==============================] - 0s 7ms/step - loss: 307.5148 - mae: 307.5148 - val_loss: 300.6439 - val_mae: 300.6439\n","Epoch 139/150\n","60/60 [==============================] - 0s 7ms/step - loss: 312.8652 - mae: 312.8652 - val_loss: 299.9839 - val_mae: 299.9839\n","Epoch 140/150\n","60/60 [==============================] - 0s 7ms/step - loss: 308.9708 - mae: 308.9708 - val_loss: 301.2594 - val_mae: 301.2594\n","Epoch 141/150\n","60/60 [==============================] - 0s 7ms/step - loss: 313.6785 - mae: 313.6786 - val_loss: 299.7520 - val_mae: 299.7520\n","Epoch 142/150\n","60/60 [==============================] - 0s 7ms/step - loss: 310.5778 - mae: 310.5778 - val_loss: 298.1846 - val_mae: 298.1846\n","Epoch 143/150\n","60/60 [==============================] - 0s 7ms/step - loss: 304.4560 - mae: 304.4560 - val_loss: 299.2658 - val_mae: 299.2657\n","Epoch 144/150\n","60/60 [==============================] - 0s 7ms/step - loss: 312.2839 - mae: 312.2839 - val_loss: 297.7914 - val_mae: 297.7914\n","Epoch 145/150\n","60/60 [==============================] - 0s 7ms/step - loss: 310.1844 - mae: 310.1844 - val_loss: 295.4910 - val_mae: 295.4910\n","Epoch 146/150\n","60/60 [==============================] - 0s 7ms/step - loss: 305.6235 - mae: 305.6235 - val_loss: 296.8527 - val_mae: 296.8527\n","Epoch 147/150\n","60/60 [==============================] - 0s 7ms/step - loss: 308.7050 - mae: 308.7050 - val_loss: 296.0976 - val_mae: 296.0976\n","Epoch 148/150\n","60/60 [==============================] - 0s 8ms/step - loss: 307.1206 - mae: 307.1206 - val_loss: 295.2796 - val_mae: 295.2796\n","Epoch 149/150\n","60/60 [==============================] - 0s 7ms/step - loss: 304.9767 - mae: 304.9767 - val_loss: 294.2274 - val_mae: 294.2274\n","Epoch 150/150\n","60/60 [==============================] - 0s 7ms/step - loss: 305.2326 - mae: 305.2326 - val_loss: 293.7924 - val_mae: 293.7924\n"]}],"source":["folds = 2\n","num_val_samples = len(predictors) // folds # floor division (i.e., round down to nearest integer.)\n","num_epochs = 150\n","batch_sizes = 75\n","all_train_mae_histories, all_val_mae_histories = [],[]  \n","\n","for i in range(folds): # the folds are going to be indexed 0 through 3 if k = 4\n","    print(\"Processing fold #:\",i)\n","    \n","    val_data = predictors[i * num_val_samples: (i + 1) * num_val_samples] \n","    val_targets = labels[i * num_val_samples: (i + 1) * num_val_samples]\n","    \n","    partial_train_data = np.concatenate(\n","        [predictors[:i * num_val_samples],\n","         predictors[(i + 1) * num_val_samples:]],\n","        axis=0)\n","    \n","    partial_train_targets = np.concatenate(\n","        [labels[:i * num_val_samples],\n","         labels[(i + 1) * num_val_samples:]],\n","        axis=0)\n","    \n","    model = build_model()\n","\n","    history = model.fit(partial_train_data, partial_train_targets,\n","                        validation_data=(val_data, val_targets),\n","                        epochs=num_epochs, batch_size=batch_sizes)\n","    \n","    train_mae_history = history.history['mae']\n","    val_mae_history = history.history['val_mae']\n","\n","    all_train_mae_histories.append(train_mae_history)\n","    all_val_mae_histories.append(val_mae_history)\n","\n","average_train_mae_history = [np.mean([x[i] for x in all_train_mae_histories]) for i in range(num_epochs)]\n","average_val_mae_history = [np.mean([x[i] for x in all_val_mae_histories]) for i in range(num_epochs)]"]},{"cell_type":"markdown","metadata":{"id":"XcD7R5-UG8sl"},"source":["Plot your average training and validation MAE loss across epochs here:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXQEVK1bHAHy"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(average_train_mae_history, c=\"r\")\n","plt.plot(average_val_mae_history,c=\"b\")\n","plt.legend(['Train','Validation'],title=\"Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Average Validation MAE Across k Folds\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"799Ss4r4cDB5"},"source":["#*Choose Final Configuration and Produce That Model Here:*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkVkAJaGcFCJ"},"outputs":[],"source":["model = build_model()\n","model.fit(predictors,labels,epochs=80, batch_size=50)"]},{"cell_type":"markdown","metadata":{"id":"MFDAd_jXiFHR"},"source":["Here's what the resulting model looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XmZBB8Y4iJER"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"9xWiloTgSuC_"},"source":["#*Final Evaluation*"]},{"cell_type":"markdown","metadata":{"id":"JVpo01AsSySZ"},"source":["Don't modify this section; this is the code I will use to evaluate that your model is output properly and that it can generate predictions on new test observations it has never seen before. If your model breaks when I feed it the new data, I will deduct marks, so please ensure that your data pre-processing function works properly!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzEJ5p5tS4Zz"},"outputs":[],"source":["from google.colab import files\n","import io\n","\n","# I'm going to upload my holdout dataset (same set of features)\n","uploaded = files.upload()\n","bluebike_holdout = pd.read_csv(io.BytesIO(uploaded['bluebikes_holdout.csv']))\n","\n","# I'm then going to pre-process it using your commands.\n","holdout_predictors, holdout_labels = processData(bluebike_holdout)\n","\n","# Then I'm going to evaluate your model's performance on that data.\n","loss_metrics = model.evaluate(holdout_predictors,holdout_labels,verbose=1)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"3.6 - Homework 1 - Solution.ipynb","provenance":[],"authorship_tag":"ABX9TyMzMTcaT3UnXcwaixWsEXVW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}